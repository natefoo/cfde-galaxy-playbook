---

global:

  default_inherits: _default_tool

destinations:

  _slurm:
    abstract: true
    runner: slurm
    params:
      tmp_dir: true
      outputs_to_working_directory: true
      metadata_strategy: extended
    scheduling:
      accept:
      - cvmfs

  slurm_conda_direct:
    inherits: _slurm
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition=normal"
    scheduling:
      require:
      - conda

  slurm_conda_direct_galaxy:
    inherits: _slurm
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition=galaxy -w {{ inventory_hostname_short }}"
    scheduling:
      require:
      - conda
      - galaxy

  slurm_apptainer:
    inherits: _slurm
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition=normal"
      singularity_enabled: true
      singularity_no_mount: null
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_singularity_default_container_id }}"
      require_container: true
    env:
    - name: APPTAINER_CACHEDIR
      value: {{ galaxy_shared_rw_root }}/tmp/apptainer_cache
    - name: APPTAINER_TMPDIR
      value: {{ galaxy_shared_rw_root }}/tmp/apptainer_tmp
    scheduling:
      accept:
      - general
      - singularity
      # for TPV shared DB
      - docker

  slurm_gxit:
    runner: slurm_pulsar
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition=normal"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      singularity_enabled: false
      docker_enabled: true
      docker_set_user: null
      docker_memory: "{mem}G"
      docker_volumes: "$job_directory:ro,$job_directory/outputs:rw,$working_directory:rw"
      container_monitor_command: "{{ job_execution_venv }}/bin/galaxy-container-monitor"
      container_monitor_result: callback
      container_resolvers:
      - type: explicit
      require_container: true
      remote_metadata: false
      transport: curl
      default_file_action: "copy"
      dependency_resolution: remote
      rewrite_parameters: true
      jobs_directory: "{{ galaxy_pulsar_embedded_staging_directory }}"
    scheduling:
      accept:
      - cvmfs
      - gxit

  ls6:
    runner: ls6
    min_accepted_cores: 1
    min_accepted_mem: 0
    context:
      time: "24:00:00"
    params:
      tmp_dir: true
      outputs_to_working_directory: false
      remote_metadata: false
      transport: curl
      dependency_resolution: local
      rewrite_parameters: true
      # corral is mounted
      #default_file_action: copy
      #default_file_action: remote_transfer
      default_file_action: remote_copy
      # using UUIDs so not separated by instance
      jobs_directory: "/scratch/10611/cfdecloud/jobs/{{ galaxy_instance_codename }}"
      file_action_config: "{{ galaxy_config_dir }}/pulsar_ls6_file_actions.yaml"
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.ls6 | join(',') }}"
      singularity_no_mount: null
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- apptainer
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
      #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time} --partition=vm-small"
      submit_native_specification: "--nodes=1 --ntasks={int(cores if force_cores else 16)} --time={time} --partition=vm-small"
    rules:
    - if: mem >= 32 or cores >= 16
      params:
        #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time} --partition=normal"
        submit_native_specification: "--nodes=1 --ntasks={int(cores if force_cores else 128)} --time={time} --partition=normal"
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - execute: '[ "$SLURM_JOB_PARTITION" = "normal" ] && export GALAXY_MEMORY_MB=253952 || export GALAXY_MEMORY_MB=28672'
    # LS6 /scratch doesn't support cross-dir hard links
    - execute: CVMFSEXEC_DIR=$(mktemp -dt cvmfsexec-XXXXXX)
    - name: CVMFSEXEC_PATH
      #value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
      value: $CVMFSEXEC_DIR/cvmfsexec
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: APPTAINER_CACHEDIR
      value: /scratch/10611/cfdecloud/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - pulsar
      - ls6

tools:

  _default_tool:
    abstract: true
    cores: 1
    mem: cores * 3.7
    context:
      time: "48:00:00"
      force_cores: false
    params:
      use_metadata_binary: true
    env:
    - name: GALAXY_VIRTUAL_ENV
      value: "{{ job_execution_venv }}"
    - name: HDF5_USE_FILE_LOCKING
      value: "FALSE"
    scheduling:
      accept:
      - general
      reject:
      - offline
    rules:
    - id: expression_tool
      if: tool.tool_type == "expression"
      params:
        use_metadata_binary: false
      env:
      - name: GALAXY_VIRTUAL_ENV
        value: $_GALAXY_VIRTUAL_ENV
      scheduling:
        require:
        - conda
        - galaxy
        reject:
        - pulsar

  _conda_direct:
    abstract: true
    scheduling:
      require:
      - conda

  _conda_direct_galaxy:
    abstract: true
    scheduling:
      require:
      - conda
      - galaxy

  upload1: {inherits: _conda_direct_galaxy}
  __DATA_FETCH__: {inherits: _conda_direct_galaxy}
  __SET_METADATA__: {inherits: _conda_direct_galaxy}
  export_remote: {inherits: _conda_direct_galaxy}
  CONVERTER_.*: {inherits: _conda_direct}

  interactive_tool_*:
    params:
      docker_host_port_cmd: /usr/local/bin/gxit-port.sh
    scheduling:
      require:
      - gxit

  interactive_tool_jupyter_notebook:
    params:
      #docker_run_extra_arguments: "-e NB_UID=$(id -u) -e NB_GID=$(id -g) -p $(/usr/local/bin/gxit-port.sh):8888"
      docker_run_extra_arguments: "-e NB_UID=$(id -u) -e NB_GID=$(id -g)"

  interactive_tool_rstudio:
    params:
      #docker_run_extra_arguments: "-e USERID=$(id -u) -e GROUPID=$(id -g) -p $(/usr/local/bin/gxit-port.sh):8780"
      docker_run_extra_arguments: "-e USERID=$(id -u) -e GROUPID=$(id -g)"
      container_override:
      - type: docker
        shell: /bin/bash
        identifier: afgane/bioc-galaxy:3.21

  Filter1:
    params:
      container_override:
      - type: singularity
        shell: /bin/sh
        identifier: /cvmfs/singularity.galaxyproject.org/all/python:3.13

  hubmap_scrna_fastqc:
    cores: 4
    mem: 3.7

  hubmap_salmon_human:
    cores: 8
    mem: 28

  hubmap_trim_reads:
    cores: 4
    mem: 3.7

  # TEST TEST TEST
  secure_hash_message_digest:
    scheduling:
      require:
      - pulsar
      - ls6
